---
layout:     post
title:      "AI(1)-Backpropagation"
subtitle:   "AI notes"
date:       2020-04-10
author:     "msgi"
header-img: "img/posts/01.jpg"
header-mask: 0.3
catalog:    true

tags:
    - Backpropagation
    - AI
    - 笔记
    - 学习
---

反向传播算法是神经网络中最基本的算法模块。 它诞生于19世纪60年代，但是直到近30年后（1989年）由Rumelhart，Hinton和Williams在一篇名为“[Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)”的论文中应用，证明了该算法的有效性，该算法才得到推广。

该算法通过链式法则有效地训练神经网络。 简言之，神经网络在每次向前传播后，反向传播算法都会执行向后传播来调整模型的参数（权重和偏差）。

**重要公式如下：**

![公式](/img/posts/bp/equ.png)

基本的公式推导如下：

$
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial z^L_j}.
\tag{1}\end{eqnarray}
$

$
\begin{eqnarray}
  \delta^L_j = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j},
\tag{2}\end{eqnarray}
$

$
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.
\tag{3}\end{eqnarray}
$

$
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j),
\tag{4}\end{eqnarray}
$

$
\begin{eqnarray}
  \delta^l_j & = & \frac{\partial C}{\partial z^l_j} \tag{5}\\\\\\\\
  & = & \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j} \tag{6}\\\\ 
  & = & \sum_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k,
\tag{7}\end{eqnarray}
$



$
\begin{eqnarray}
  \delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma'(z^l_j).
\tag{10}\end{eqnarray}
$
