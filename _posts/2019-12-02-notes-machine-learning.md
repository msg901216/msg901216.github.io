---
layout:     post
title:      "ml学习笔记"
subtitle:   "machine learning notes"
date:       2018-12-02
author:     "msg"
header-img: "img/posts/04.jpg"
header-mask: 0.3
catalog:    true

tags:
    - machine learning
    - ubuntu
    - 学习
    - 转载
---

### Baseline

Baseline Model**是机器学习领域的一个术语，简单来说，就是使用最普遍的情况来做结果预测。比如一个猜硬币正反面游戏，最朴素的策略就是永远猜正（或者永远猜反），这样你至少有50%的准确率。再比如说，很多学习不好的同学，应该都做过这件事情：考试里选择题一率选C，这也是利用了Baseline Model来“预测”问题的结果。理论上，这样做至少能拿25%的分数。而之所以选择C，是发现，似乎选择C，能拿到比25%还多的分数。毕竟，选择题的答案不是真正的随机分布的。有意思的是，在极个别的情况下，这样做的考分比另一部分同学正儿八经地答题，得分还高。

### dropout

***dropout*** 作为一种预防CNN过拟合的正则化方法被Hinton等人在2012年的经典论文《ImageNet Classification with Deep Convolutional》中提出。dropout的原理很简单：在一次训练时的迭代中，对每一层中的神经元（总数为N）以概率P随机剔除，用余下的（1-P）×N个神经元所构成的网络来训练本次迭代中的数据（batchsize个样本）。

对于Dropout为什么可以减少overfitting的原因如下：
一般情况下，对于同一组训练数据，利用不同的神经网络训练之后，求其输出的平均值可以减少overfitting。Dropout就是利用这个原理，每次丢掉一半的一隐藏层神经元，相当于在不同的神经网络上进行训练，这样就减少了神经元之间的依赖性，即每个神经元不能依赖于某几个其他的神经元（指层与层之间相连接的神经元），使神经网络更加能学习到与其他神经元之间的更加健壮robust的特征。在Dropout的作者文章中，测试手写数字的准确率达到了98.7%!所以Dropout不仅减少overfitting，还能提高准确率。

### 正则化

***正则化*** 是用来降低overfitting（过拟合）的，减少过拟合的的其他方法有：增加训练集数量。对于数据集梳理有限的情况下，防止过拟合的另外一种方式就是降低模型的复杂度，怎么降低?一种方式就是在cost函数中加入正则化项，正则化项可以理解为复杂度，cost越小越好，但cost加上正则项之后，为了使cost小，就不能让正则项变大，也就是不能让模型更复杂，这样就降低了模型复杂度，也就降低了过拟合。这就是正则化。正则化也有很多种，常见为两种L2和L1。

Regularization能降低overfitting的原因：

在神经网络中，正则化网络更倾向于小的权重，在权重小的情况下，数据x随机的变化不会对神经网络的模型造成太大的影响，所以可能性更小的受到数据局部噪音的影响。而未加入正则化的神经网络，权重大，容易通过较大的模型改变来适应数据，更容易学习到局部的噪音。

### 序列检测

实现序列预测有很多不同的方法，比如利用机器学习中的马尔科夫模型/有向图，深度学习领域中的RNN/LSTM等等。紧凑预测树（Compact Prediction Tree，即CPT）的算法的性能比很多非常知名的方法还要强大，比如前面提到的马尔科夫模型和有向图。

每当我们想预测一个事件之后可能会发生另一个特定事件时，就需要用到序列预测。

目前解决序列预测最常用的方法是LSTM和RNN，它们已经成为序列建模的热门选择，用于文本、音频等等。不过，它们却有两个基本的问题：

> 训练时间很长，往往要几十个小时
> 如果序列中包含了之前训练迭代中未见过的项时，就需要重新训练它们。这个过程代价很高昂，在频繁出现新项目的问题中，就无法使用它们。

紧凑预测树（CPT）这种算法在处理序列预测问题时，往往比传统机器学习方法比如马尔科夫模型和深度学习方法比如自动编码器更加准确。

CPT的一大卖点就是其快速的训练和预测时间。

