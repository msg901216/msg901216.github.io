---
layout:     post
title:      "FastText"
subtitle:   "FastText学习"
date:       2018-10-11
author:     "msg"
header-img: "img/posts/01.jpg"
header-mask: 0.3
catalog:    true

tags:
    - python
    - 学习
    - 自然语言处理
    - fastText
    - 转载
---

> 用到了fastText，记录一下。这里记录的是Facebook的fastText工具包，实现了词向量和分类，因为要用Python，所以github找到一个封装了fasttext的Python包，实现上就用这个。

### fastText介绍

不同于word2vec, fasttext利用的是词的形态学信息，也就是词的内部构造信息，也就是子词信息。

那什么是子词信息？fasttext采用的character n-gram来做的，比如where这个词，那么它的character 3-gram 子词包含如下<wh, whe, her, ere, re>以及本身<where>。

这对尖括号可以方便的将her这个单词与where的子词her进行区分，her的character 3-gram子词不包含 her，于是这两个便可以区分开来。

那么为什么要利用子词信息呢？脸书的研究者们认为，像word2vec这类词分布表示模型，词与词之间的信息没有更好的共享，也就是参数没有得到有效的共享，分解为粒度更小的子词后，通过共享子词表示，来达到信息共享的目的。

### 原理

给定一个character n-gram 字典，假设大小为G，并且每个子词都有自己的词向量表示，那么词w的词向量，可以由构成它的所有子词对应的向量求和来表示。另一点，与word2vec不一样的是，fasttext使用的分类的方法，也就是根据与它计算score的另一个词是否是上下文来进行二分类，具体用到的是logistics 回归方法。

根据上面这些描述，可以发现一些端倪

fasttext对罕见词非常有利，因为罕见词罕见是本身出现的次数足够少，但是构成其的character n-gram肯定比词本身出现的次数多，由于这些子词是共享的，因此可以从高频词中受益。

其次，对于OOV(out of vocabulary)问题， 由于一个词可以被拆分成多个子词，当前词OOV，其大部分子词讲道理不会OOV，因此利用这些没有OOV的子词，可以在一定程度上缓解OOV问题。

### 代码示例

#### 安装(python)

```python

pip install fasttext

```

#### 训练词向量

```python

import fasttext

# Skipgram model
model = fasttext.skipgram('data.txt', 'model')
print model.words # list of words in dictionary

# CBOW model
model = fasttext.cbow('data.txt', 'model')
print model.words # list of words in dictionary

```

训练好后，可以得到两个文件，model.bin 和 model.vec，model.bin保存了所有的词以及向量信息，后期可以用来继续训练，model.vec只保存了向量信息。

用训练好的词向量，可以直接得到词的向量

```python
print model['king'] # get the vector of the word 'king'
```

#### 加载词向量

```python

model = fasttext.load_model('model.bin')
print model.words # list of words in dictionary
print model['king'] # get the vector of the word 'king'

```

#### 文本分类

```python
classifier = fasttext.supervised('data.train.txt', 'model')
```
可以指明类别标签

```python
classifier = fasttext.supervised('data.train.txt', 'model', label_prefix='__label__')
```

训练好后，得到model.bin和model.vec文件，训练好后，可以根据训练好的模型，测试效果。

```python
result = classifier.test('test.txt')
```

可以推理某个文本所属的类别

```python
texts = ['example very long text 1', 'example very longtext 2']
labels = classifier.predict(texts)
print(labels)

# Or with the probability
labels = classifier.predict_proba(texts)
print(labels)
```

可以指明前K个可能所属的类别

```python
labels = classifier.predict(texts, k=3)
print(labels)

# Or with the probability
labels = classifier.predict_proba(texts, k=3)
print(labels)
```

#### 模型参数

***1) skipgram***

```python
model = fasttext.skipgram(params)
```

params的参数如下

```
input_file     training file path (required)
output         output file path (required)
lr             learning rate [0.05]
lr_update_rate change the rate of updates for the learning rate [100]
dim            size of word vectors [100]
ws             size of the context window [5]
epoch          number of epochs [5]
min_count      minimal number of word occurences [5]
neg            number of negatives sampled [5]
word_ngrams    max length of word ngram [1]
loss           loss function {ns, hs, softmax} [ns]
bucket         number of buckets [2000000]
minn           min length of char ngram [3]
maxn           max length of char ngram [6]
thread         number of threads [12]
t              sampling threshold [0.0001]
silent         disable the log output from the C++ extension [1]
encoding       specify input_file encoding [utf-8]
```
例如：

```python
model = fasttext.skipgram('train.txt', 'model', lr=0.1, dim=300)
```

***2) cbow***

```python
model = fasttext.cbow(params)
```

params列表如下：

```
input_file     training file path (required)
output         output file path (required)
lr             learning rate [0.05]
lr_update_rate change the rate of updates for the learning rate [100]
dim            size of word vectors [100]
ws             size of the context window [5]
epoch          number of epochs [5]
min_count      minimal number of word occurences [5]
neg            number of negatives sampled [5]
word_ngrams    max length of word ngram [1]
loss           loss function {ns, hs, softmax} [ns]
bucket         number of buckets [2000000]
minn           min length of char ngram [3]
maxn           max length of char ngram [6]
thread         number of threads [12]
t              sampling threshold [0.0001]
silent         disable the log output from the C++ extension [1]
encoding       specify input_file encoding [utf-8]
```

例如：

```python
model = fasttext.cbow('train.txt', 'model', lr=0.1, dim=300)
```

***3) 加载预训练模型***

```python
model = fasttext.load_model('model.bin', encoding='utf-8')
```

***4) 词向量模型的参数***

```
model.model_name       # Model name
model.words            # List of words in the dictionary
model.dim              # Size of word vector
model.ws               # Size of context window
model.epoch            # Number of epochs
model.min_count        # Minimal number of word occurences
model.neg              # Number of negative sampled
model.word_ngrams      # Max length of word ngram
model.loss_name        # Loss function name
model.bucket           # Number of buckets
model.minn             # Min length of char ngram
model.maxn             # Max length of char ngram
model.lr_update_rate   # Rate of updates for the learning rate
model.t                # Value of sampling threshold
model.encoding         # Encoding of the model
model[word]            # Get the vector of specified word
```

***5) 有监督训练***

```python
classifier = fasttext.supervised(params)
```

params如下：

```
input_file     			training file path (required)
output         			output file path (required)
label_prefix   			label prefix ['__label__']
lr             			learning rate [0.1]
lr_update_rate 			change the rate of updates for the learning rate [100]
dim            			size of word vectors [100]
ws             			size of the context window [5]
epoch          			number of epochs [5]
min_count      			minimal number of word occurences [1]
neg            			number of negatives sampled [5]
word_ngrams    			max length of word ngram [1]
loss           			loss function {ns, hs, softmax} [softmax]
bucket         			number of buckets [0]
minn           			min length of char ngram [0]
maxn           			max length of char ngram [0]
thread         			number of threads [12]
t              			sampling threshold [0.0001]
silent         			disable the log output from the C++ extension [1]
encoding       			specify input_file encoding [utf-8]
pretrained_vectors		pretrained word vectors (.vec file) for supervised learning []
```

示例：

```python
classifier = fasttext.supervised('train.txt', 'model', label_prefix='__myprefix__',
                                 thread=4)
```

***6) 加载训练模型***

```python
classifier = fasttext.load_model('classifier.bin', label_prefix='some_prefix')
```

***7) 测试分类***

```python
result = classifier.test(params)

# Properties
result.precision # Precision at one
result.recall    # Recall at one
result.nexamples # Number of test examples
```

***8) 推理分类***

```python
labels = classifier.predict(texts, k)

# Or with probability
labels = classifier.predict_proba(texts, k)
```

***9) 分类器参数***

```
classifier.labels                  # List of labels
classifier.label_prefix            # Prefix of the label
classifier.dim                     # Size of word vector
classifier.ws                      # Size of context window
classifier.epoch                   # Number of epochs
classifier.min_count               # Minimal number of word occurences
classifier.neg                     # Number of negative sampled
classifier.word_ngrams             # Max length of word ngram
classifier.loss_name               # Loss function name
classifier.bucket                  # Number of buckets
classifier.minn                    # Min length of char ngram
classifier.maxn                    # Max length of char ngram
classifier.lr_update_rate          # Rate of updates for the learning rate
classifier.t                       # Value of sampling threshold
classifier.encoding                # Encoding that used by classifier
classifier.test(filename, k)       # Test the classifier
classifier.predict(texts, k)       # Predict the most likely label
classifier.predict_proba(texts, k) # Predict the most likely label include their probability
```
