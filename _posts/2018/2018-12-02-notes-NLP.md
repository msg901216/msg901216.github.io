---
layout:     post
title:      "NLP学习笔记"
subtitle:   "学习笔记"
date:       2018-12-02
author:     "msg"
header-img: "img/posts/04.jpg"
header-mask: 0.3
catalog:    true

tags:
    - NLP
    - 学习
    - 自然语言处理
    - 转载
---

### 对话系统

 > 智能问答系统：智能问答系统越来越受到重视，工作中也一直在一线进行架构设计和编码工作。如何从零开始构建一个完整的智能问答系统，如今也有了一点点经验。目前主要从事智能客服方面的工作，最近看了一篇文章，下面记录一下。

对话系统分为任务导向型（task-oriented)对话系统和非任务导向型(non-task-oriented)对话系统(也称为聊天机器人)。一个完整的对话系统包括的技术内容如下：

![对话系统](/img/posts/dialog.jpg)

#### 任务导向型

1) 面向任务的系统旨在帮助用户完成实际具体的任务，例如帮助用户找寻商品，预订酒店餐厅，询问天气，播放音乐等。 

2) 面向任务的系统广泛采用的方法是将对话响应视为一条管道（pipeline)。系统首先理解人类所传达的信息，将其作为一种内部状态，然后根据对话状态的策略采取一系列相应的行为，最后将动作转化为自然语言的表现形式。

3) 虽然语言理解是通过统计模型来处理的，但是大多数已经部署的对话系统仍然使用***手工的特性或手工制定的规则***，用于状态和动作空间表示、意图检测和插槽填充。

#### 非任务导向型

1) 非任务导向的对话系统与人类交互，提供合理的回复和娱乐消遣功能，通常情况下主要集中在开放的领域与人交谈。虽然非任务导向的系统似乎在进行聊天，但是它在许多实际应用程序中都发挥了作用。

2) 数据显示，在网上购物场景中，近***80%***的话语是聊天信息，处理这些问题的方式与用户体验密切相关。

3) 非任务导向型主要有两种方法：

* 生成方法，例如序列到序列模型（seq2seq），在对话过程中产生合适的回复，**生成型聊天机器人**目前是研究界的一个热点，和检索型聊天机器人不同的是，它可以生成一种全新的回复，因此相对更为灵活，但它也有自身的缺点，比如有时候会出现语法错误，或者生成一些没有意义的回复；
* 基于检索的方法，从事先定义好的索引中进行搜索，学习从当前对话中选择回复。检索型方法的缺点在于它过于依赖数据质量，如果选用的数据质量欠佳，那就很有可能前功尽弃。 

### 语言模型

语言模型（Language Model）的作用是为一个长度为 m 的文本确定一个概率分布P，表示这段文本存在的可能性。

在实践中，如果文本的长度较长，$P(wi | w_1, w_2, . . . , w_{i−1}$的估算会非常困难。因此，研究者们提出使用一个简化模型：n元模型（n-gram model）。在n元模型中估算条件概率时，只需要对当前词的前n个词进行计算。在n元模型中，传统的方法一般采用频率计数的比例来估算n元条件概率。当n较大时，会存在数据稀疏问题，导致估算结果不准确。因此，一般在百万词级别的语料中，一般也就用到三元模型。

### Monte Carlo Method

蒙特卡罗方法是一种计算方法。原理是通过大量随机样本，去了解一个系统，进而得到所要计算的值。

它非常强大和灵活，又相当简单易懂，很容易实现。对于许多问题来说，它往往是最简单的计算方法，有时甚至是唯一可行的方法。

它诞生于上个世纪40年代美国的"曼哈顿计划"，名字来源于赌城蒙特卡罗，象征概率。

### 计算相似度的方法

度量文本相似度包括如下三种方法：

一是基于关键词匹配的传统方法，如N-gram相似度；

二是将文本映射到向量空间，再利用余弦相似度等方法；

三是深度学习的方法，如基于用户点击数据的深度学习语义匹配模型DSSM，基于卷积神经网络的ConvNet，以及目前state-of-art的Siamese LSTM等方法。 

1) 字面距离

> 莱文斯坦距离(编辑距离)、Jaro距离、SimHash

2) 语义相似性

①基于关键词匹配

> N-gram 相似度\
> Jaccard 相似度

② 基于向量空间

> Word2vec\
> TF-IDF\
> LSA\
> 相似度计算：欧式距离、曼哈顿距离、余弦相似度、其他

③基于深度学习

> 深度学习\
> DSSM\
> ConvNet\
> Skip-thoughts Vectors\
> Tree-LSTM\
> Siamese LSTM/Manhattan LSTM/MaLSTM\
> Others

### 算法的提出

一个算法的提出，一定是先有场景和需求，或者是前面的算法有改进的空间。场景高于算法这点是毋庸置疑的。

### 特征工程

#### 词袋模型

词袋模型的基本思想是将文本符号化，将一段文本表示成一堆符号的集合；由于中文文本的多样性，通常导致构建的词袋维数较大，仅仅以词为单位（Unigram）构建的词袋可能就达到几万维，如果考虑二元词组（Bigram）、三元词组（Trigram）的话词袋大小可能会有几十万之多，因此基于词袋模型的特征表示通常是极其稀疏的。

几种不同的表示方法：

第1种：Naive版本，不考虑词出现的频率，只要出现过就在相应的位置标1，否则为0；

第2种：考虑词频（即term frequency），认为一段文本中出现越多的词越重要，因此权重也越大；

第3种：考虑词的重要性，以TFIDF表征一个词的重要程度。TFIDF反映了一种折中的思想：即在一篇文档中，TF认为一个词出现的次数越大可能越重要（但也可能并不是，比如停用词：“的”“是”之类的）；IDF认为一个词出现在的文档数越少越重要（但也可能不是，比如一些无意义的生僻词）。

通常情况下采用第3种方法。文本中所出现的词的重要程度是不太一样的，比如上面的例子中“我”，“喜欢”，“学习”这3个词就要比其他词更为重要。除了TFIDF的表征方法，还有chi-square，互信息（MI），熵等其他一些衡量词重要性的指标（见这里）。

经验总结：

通常考虑unigram和bigram来构建词袋模型（trigram维数太高，取得的gain也不高）；

用TFIDF时，注意对TF作归一化，通常用词频除以文本的长度；

如果构建的词袋维数太高，可以用TF（或者TFIDF）来卡，将一些不常见的词（会有很多噪音词，如联系方式、邮箱之类的）过滤掉；

如果有一些先验的词袋，word count通常都是比较强的一维特征（比如情感分类中，正负情感词的出现次数），可以考虑；

基于词袋模型构建的特征通常高维但稀疏，通常使用非线性模型取得的效果较线性的要好，推荐大家尝试使用一些基于决策树的boosting模型，如GBDT；这也很好理解，较线性模型而言，非线性模型能够学习出更加复杂的规则，对于文本而言，体现在能够一定程度上考虑词出现的语境（context）情况，比如，对于识别文本是否为骂人语料，文本中出现“妈”，同时也出现“你”，那么为骂人的概率会增大。

词袋模型比较简单直观，它通常能学习出一些关键词和类别之间的映射关系，但是缺点也很明显：

丢失了文本中词出现的先后顺序信息；

仅将词语符号化，没有考虑词之间的语义联系（比如，“麦克风”和“话筒”是不同的词，但是语义是相同的）；

#### embedding

word2vec的原理很简单，基本思想是用词出现的上下文来表示这个词，上下文越接近的词之间的语义相似性越高。例如，上一小节中举到的例子，“话筒”和“麦克风”两者的上下文可能非常接近，因此会被认为是语义接近的。（不过语义接近并不代表含义接近，例如“黑色”和“白色”的上下文是相似的，但所代表的含义可能却是相反的）。

目前做word embedding的方法很多，比较流行的有两种：word2vec、GloVe

word2vec和GloVe两者的思想是类似的，都是用词的上下文来表示这个词，但是用的方法不同：word2vec是predict-based，用一个3层的NN模型来预测词的上下文（或者反过来），词向量是训练过程的中间产物；而GloVe则是count-based的方法，通过对共现词矩阵做降维来获取词的向量。两者在效果上相差不大，但GloVe模型的优势在于矩阵运算可以并行化，这样训练速度能加快。具体两者的差别可以看[论文](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf)。

有了word embedding之后，我们怎么得到文本的embedding呢？

对于短文本而言，比较好的方法有：

(1) 取短文本的各个词向量之和（或者取平均）作为文本的向量表示；

(2) 用一个pre-train好的NN model得到文本作为输入的最后一层向量表示；

除此之外，还有TwitterLda，TwitterLda是Lda的简化版本，针对短文本做主题刻画，实际效果也还不错。

基于embedding的特征刻画的是语义、主题层面上的特征，较词匹配而言，有一定的泛化能力。

#### NN model

NN的好处在于能end2end实现模型的训练和测试，利用模型的非线性和众多参数来学习特征，而不需要手工提取特征。

CNN和RNN都是NLP中常用的模型，两个模型捕捉特征的角度也不太一样，CNN善于捕捉文本中关键的局部信息，而RNN则善于捕捉文本的上下文信息（考虑语序信息），并且有一定的记忆能力，两者都可以用在文本分类任务中，而且效果都不错。

对于简单的文本分类任务，用几个简单的NN模型基本就够了（调参数也是一大累活儿）。

最后我们可以将这些NNs预测的分值作为我们分类系统的一个特征，来加强分类系统的性能。

#### 任务本身

通过我们对数据的观察和感知，也许能够发现一些可能有用的特征。有时候，这些手工特征对最后的分类效果提升很大。举个例子，比如对于正负面评论分类任务，对于负面评论，包含负面词的数量就是一维很强的特征。

这部分的特征设计就是在拼脑力和拼经验，建议可以多看看各个类别数据找找感觉，将那些你直观上感觉对分类有帮助的东西设计成特征，有时候这些经验主义的东西很有用（可能是模型从数据学习不出来的）。

#### 特征融合

在设计完这些特征之后，怎么融合更合适呢？对于特征维数较高、数据模式复杂的情况，建议用非线性模型（如比较流行的GDBT, XGBoost）；对于特征维数较低、数据模式简单的情况，建议用简单的线性模型即可（如LR）。

模型融合能够从多个角度更加全面地学习出训练数据中的模式，往往能比单个模型效果好一点（2~3个点左右）。

另外，通过观察LR模型给各个特征分配的权重大小和正负，我们可以看出对于训练数据而言，这些特征影响分类的重要程度（权重大小（绝对值）），以及特征影响最终分类目标的极性。特别的，我们可以通过观察那些手工特征的权重来验证这些特征的有效性和有效程度。

### 自然语言处理学什么？

这门学科的知识可是相当的广泛，广泛到你不需要掌握任何知识就可以直接学，因为你不可能掌握它依赖的全部知识。

第一难：语言学。有的在研究语言描述的学问，有的在研究语言理论的学问，有的在研究不同语言对比的学问，有的在研究语言共同点上的学问，有的在研究语言发展的历史，有的在研究语言的结构，总之用一个字来形容那是一个涉猎广泛啊

第二难：语音学。语音学研究领域分三块：一块是研究声音是怎么发出来的；一块是研究声音是怎么传递的；一块是研究声音是怎么接收的。

第三难：概率论。说话是一个概率问题，我经常说“尼玛”，那我再次说“尼玛”的概率就高。提到概率论那就少不了这些：贝叶斯老爷爷、马尔可夫大叔……

第四难：信息论。香农老爷爷提出的熵理论影响那是相当巨大啊，没有它估计就没有我们计算机人事什么事了，因为没有他就没有互联网了。还有人说没有图灵就没有计算机了。

第五难：机器学习。机器学习啊——得好好学

第六难：形式语言与自动机。形式语言就是把语言搞的很形式，换句话说就是本来你能懂的东西，搞成你不能懂的东西，那就是形式语言啦！短语结构语言、上下文有关语言、上下文无关语言、正则语言。自动机包括：图灵机、有穷自动机、下推自动机、线性有界自动机，自动机英文叫automata，表达的是自动形成一些信息，也就是说根据前面能自动判断出后面。形式语言用在自然语言处理上我理解，都有语言俩字，可是这自动机有什么用呢？这您还真问着了，您见过拼写检查吗？这就是用的自动机，用处杠杠的！

第七难：语言知识库。互联网上有无数文本内容，用我们抽象的话说那都是知识，但是简单放在电脑里那就是一串字符串，怎么才能让它以知识的形式存储呢？首先得让计算机能分析语言，那么语料就是它学习的基础、是种子，然后有了基础再让它把语言里的知识存储起来，这就形成了语言知识库。

第八难：语言模型。模型顾名思义就是“模子”，就是“往上靠”的意思，怎么靠上去更吻合就怎么靠，这就是语言模型。用形式化的语言再来说一下：把很多已经整理好的模子放在那里，遇到一个新内容的时候看看属于哪种格式，那就按照这种模子来解释。

第九难：分词、实体识别、词性标注。这部分开始纯语言处理了，前几节也简单讲过这部分内容，分词就是让计算机自动区分出汉字组成的词语，因为一个词语一个意思嘛。实体识别就是再分词之后能够根据各种短语形式判断出哪个词表示的是一个物体或组织或人名或……。词性标注就是给你一句话，你能识别出“名动形、数量代、副介连助叹拟声”。

第十难：句法分析。句法分析类似于小学时学的主谓宾补定状的区分，只是要在这基础上组合成短语，也就是把一个非结构化的句子分析称结构化的数据结构

第十一难：语义分析。看起来一步一步越来越深入了。语义是基于句法分析，进一步理解句子的意思，最重要的就是消除歧义，人姑且还会理解出歧义来呢，何况一个机器

第十二难：篇章分析。一堆堆的句子，每个都分析明白了，但是一堆句子组合成的篇章又怎么才能联系起来呢？你得总结出本文的中心思想不是？这他娘的是小学语文里最难的一道题。

### 自然语言处理和聊天机器人什么关系？

第一个应用：机器翻译。

第二个应用：语音翻译。

第三个应用：文本分类与情感分析。

第四个应用：信息检索与问答系统。

第五个应用：自动文摘和信息抽取。

第六个应用：人机对话。融合了语音识别、口语情感分析、再加上问答系统的全部内容，是自然语言处理的最高境界，离机器人统霸世界不远了。

还有：文本挖掘、舆情分析、隐喻计算、文字编辑和自动校对、作文自动评分、OCR、说话人识别验证……


这几个博客[佟学强](http://www.cnblogs.com/txq157)、[红色石头](https://blog.csdn.net/red_stone1/)、[科学空间](https://spaces.ac.cn/)、[liuchongee](https://blog.csdn.net/liuchonge)、[Jey Zhang](http://www.jeyzhang.com/)、[西士城](https://zhuanlan.zhihu.com/xitucheng10)、[机器学习算法与自然语言处理](https://zhuanlan.zhihu.com/qinlibo-ml)、[悟乙己](https://blog.csdn.net/sinat_26917383/article/details/54882554)、[wildml](http://www.wildml.com/)、[模型融合](https://zhuanlan.zhihu.com/p/33589222)很不错